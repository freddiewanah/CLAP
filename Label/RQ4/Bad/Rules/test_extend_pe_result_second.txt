#Method to be tested:
def extend_pe(self, x):
    """Reset the positional encodings."""
    if self.pe is not None:
        if self.pe.size(1) >= x.size(1):
            if self.pe.dtype != x.dtype or self.pe.device != x.device:
                self.pe = self.pe.to(dtype=x.dtype, device=x.device)
            return
    pe = torch.zeros(x.size(1), self.d_model)
    if self.reverse:
        position = torch.arange(x.size(1) - 1, -1, -1.0, dtype=torch.float32).unsqueeze(1)
    else:
        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / self.d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0)
    self.pe = pe.to(device=x.device, dtype=x.dtype)
#Unit test:

def test_extend_pe(self):
    inp = self.sample.transpose(0, 1)
    self.rel_pos_enc.extend_pe(inp)
    expected_pe = torch.tensor([[[0.1411, -0.99], [0.9093, -0.4161], [0.8415, 0.5403], [0.0, 1.0], [-0.8415, 0.5403], [-0.9093, -0.4161], [-0.1411, -0.99]]])
    "<AssertPlaceholder1>"


#Generated assertions:
self.assertTrue(torch.allclose(self.rel_pos_enc.pe, expected_pe))


----------
    self.assertTrue(np.allclose(expected_pe.cpu().detach().numpy(), self.rel_pos_enc.pe.cpu().detach().numpy(), atol=0.0001))