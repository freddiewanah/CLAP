def run(self, args, opts):
    if len(args) < 1:
        raise UsageError()
    elif len(args) > 1:
        raise UsageError("running 'scrapy crawl' with more than one spider is not supported")
    spname = args[0]
    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)
    if getattr(crawl_defer, 'result', None) is not None and issubclass(crawl_defer.result.type, Exception):
        self.exitcode = 1
    else:
        self.crawler_process.start()
        if self.crawler_process.bootstrap_failed or (hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception):
            self.exitcode = 1

----------

@defer.inlineCallbacks
def test_crawler_dupefilter(self):
    run = CrawlerRun(TestDupeFilterSpider)
    yield run.run()
    self._assert_scheduled_requests(run, count=8)
    self._assert_dropped_requests(run)

----------

def _assert_scheduled_requests(self, run: CrawlerRun, count=None):
    self.assertEqual(count, len(run.reqplug))
    paths_expected = ['/item999.html', '/item2.html', '/item1.html']
    urls_requested = {rq[0].url for rq in run.reqplug}
    urls_expected = {run.geturl(p) for p in paths_expected}
    assert urls_expected <= urls_requested
    scheduled_requests_count = len(run.reqplug)
    dropped_requests_count = len(run.reqdropped)
    responses_count = len(run.respplug)
    self.assertEqual(scheduled_requests_count, dropped_requests_count + responses_count)
    self.assertEqual(len(run.reqreached), responses_count)
def _assert_dropped_requests(self, run: CrawlerRun):
    self.assertEqual(len(run.reqdropped), 1)

Test Class Name: EngineTest