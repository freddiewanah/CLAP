@defer.inlineCallbacks
def crawl(self, *args, **kwargs):
    if self.crawling:
        raise RuntimeError('Crawling already taking place')
    self.crawling = True
    try:
        self.spider = self._create_spider(*args, **kwargs)
        self.engine = self._create_engine()
        start_requests = iter(self.spider.start_requests())
        yield self.engine.open_spider(self.spider, start_requests)
        yield defer.maybeDeferred(self.engine.start)
    except Exception:
        self.crawling = False
        if self.engine is not None:
            yield self.engine.close()
        raise

----------

@defer.inlineCallbacks
def test_graceful_crawl_error_handling(self):
    """
        Test whether errors happening anywhere in Crawler.crawl() are properly
        reported (and not somehow swallowed) after a graceful engine shutdown.
        The errors should not come from within Scrapy's core but from within
        spiders/middlewares/etc., e.g. raised in Spider.start_requests(),
        SpiderMiddleware.process_start_requests(), etc.
        """

    class TestError(Exception):
        pass

    class FaultySpider(SimpleSpider):

        def start_requests(self):
            raise TestError
    crawler = get_crawler(FaultySpider)
    yield self.assertFailure(crawler.crawl(mockserver=self.mockserver), TestError)
    self.assertFalse(crawler.crawling)

----------



Test Class Name: CrawlTestCase