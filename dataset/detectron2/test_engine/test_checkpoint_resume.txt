def resume_or_load(self, resume=True):
    """
        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by
        a `last_checkpoint` file), resume from the file. Resuming means loading all
        available states (eg. optimizer and scheduler) and update iteration counter
        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.

        Otherwise, this is considered as an independent training. The method will load model
        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start
        from iteration 0.

        Args:
            resume (bool): whether to do resume or not
        """
    self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)
    if resume and self.checkpointer.has_checkpoint():
        self.start_iter = self.iter + 1

----------

def test_checkpoint_resume(self):
    model = _SimpleModel()
    dataloader = self._data_loader('cpu')
    opt = torch.optim.SGD(model.parameters(), 0.1)
    scheduler = torch.optim.lr_scheduler.StepLR(opt, 3)
    with tempfile.TemporaryDirectory(prefix='detectron2_test') as d:
        trainer = SimpleTrainer(model, dataloader, opt)
        checkpointer = Checkpointer(model, d, opt=opt, trainer=trainer)
        trainer.register_hooks([hooks.LRScheduler(scheduler=scheduler), hooks.PeriodicCheckpointer(checkpointer, 10)])
        trainer.train(0, 12)
        self.assertAlmostEqual(opt.param_groups[0]['lr'], 1e-05)
        self.assertEqual(scheduler.last_epoch, 12)
        del trainer
        opt = torch.optim.SGD(model.parameters(), 999)
        trainer = SimpleTrainer(model, dataloader, opt)
        scheduler = torch.optim.lr_scheduler.StepLR(opt, 3)
        trainer.register_hooks([hooks.LRScheduler(scheduler=scheduler)])
        checkpointer = Checkpointer(model, d, opt=opt, trainer=trainer)
        checkpointer.resume_or_load('non_exist.pth')
        self.assertEqual(trainer.iter, 11)
        self.assertEqual(scheduler.last_epoch, 12)
        self.assertAlmostEqual(opt.param_groups[0]['lr'], 1e-05)

----------

def _data_loader(self, device):
    device = torch.device(device)
    while True:
        yield torch.rand(3, 3).to(device)

Test Class Name: TestTrainer