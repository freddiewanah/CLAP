def train(self):
    """
        Run training.

        Returns:
            OrderedDict of results, if evaluation is enabled. Otherwise None.
        """
    super().train(self.start_iter, self.max_iter)
    if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():
        assert hasattr(self, '_last_eval_results'), 'No evaluation results obtained during training!'
        verify_results(self.cfg, self._last_eval_results)
        return self._last_eval_results

----------

def test_best_checkpointer(self):
    model = _SimpleModel()
    dataloader = self._data_loader('cpu')
    opt = torch.optim.SGD(model.parameters(), 0.1)
    metric_name = 'metric'
    total_iter = 40
    test_period = 10
    test_cases = [('max', iter([0.3, 0.4, 0.35, 0.5]), 3), ('min', iter([1.0, 0.8, 0.9, 0.9]), 2), ('min', iter([math.nan, 0.8, 0.9, 0.9]), 1)]
    for (mode, metrics, call_count) in test_cases:
        trainer = SimpleTrainer(model, dataloader, opt)
        with tempfile.TemporaryDirectory(prefix='detectron2_test') as d:
            checkpointer = Checkpointer(model, d, opt=opt, trainer=trainer)
            trainer.register_hooks([hooks.EvalHook(test_period, lambda : {metric_name: next(metrics)}), hooks.BestCheckpointer(test_period, checkpointer, metric_name, mode=mode)])
            with mock.patch.object(checkpointer, 'save') as mock_save_method:
                trainer.train(0, total_iter)
                self.assertEqual(mock_save_method.call_count, call_count)

----------

def _data_loader(self, device):
    device = torch.device(device)
    while True:
        yield torch.rand(3, 3).to(device)

Test Class Name: TestTrainer