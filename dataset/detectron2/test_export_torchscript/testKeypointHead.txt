@contextmanager
def patch_builtin_len(modules=()):
    """
    Patch the builtin len() function of a few detectron2 modules
    to use __len__ instead, because __len__ does not convert values to
    integers and therefore is friendly to tracing.

    Args:
        modules (list[stsr]): names of extra modules to patch len(), in
            addition to those in detectron2.
    """

    def _new_len(obj):
        return obj.__len__()
    with ExitStack() as stack:
        MODULES = ['detectron2.modeling.roi_heads.fast_rcnn', 'detectron2.modeling.roi_heads.mask_head', 'detectron2.modeling.roi_heads.keypoint_head'] + list(modules)
        ctxs = [stack.enter_context(mock.patch(mod + '.len')) for mod in MODULES]
        for m in ctxs:
            m.side_effect = _new_len
        yield

----------

def testKeypointHead(self):

    class M(nn.Module):

        def __init__(self):
            super().__init__()
            self.model = KRCNNConvDeconvUpsampleHead(ShapeSpec(channels=4, height=14, width=14), num_keypoints=17, conv_dims=(4,))

        def forward(self, x, predbox1, predbox2):
            inst = [Instances((100, 100), pred_boxes=Boxes(predbox1)), Instances((100, 100), pred_boxes=Boxes(predbox2))]
            ret = self.model(x, inst)
            return tuple((x.pred_keypoints for x in ret))
    model = M()
    model.eval()

    def gen_input(num1, num2):
        feat = torch.randn((num1 + num2, 4, 14, 14))
        box1 = random_boxes(num1)
        box2 = random_boxes(num2)
        return (feat, box1, box2)
    with torch.no_grad(), patch_builtin_len():
        trace = torch.jit.trace(model, gen_input(15, 15), check_trace=False)
        inputs = gen_input(12, 10)
        trace_outputs = trace(*inputs)
        true_outputs = model(*inputs)
        for (trace_output, true_output) in zip(trace_outputs, true_outputs):
            self.assertTrue(torch.allclose(trace_output, true_output))

----------



Test Class Name: TestTracing