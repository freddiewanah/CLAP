def minimize(self, loss, var_list, tape=None):
    """Minimize `loss` by updating `var_list`.

        This method simply computes gradient using `tf.GradientTape` and calls
        `apply_gradients()`. If you want to process the gradient before applying
        then call `tf.GradientTape` and `apply_gradients()` explicitly instead
        of using this function.

        Args:
          loss: `Tensor` or callable. If a callable, `loss` should take no
            arguments and return the value to minimize.
          var_list: list or tuple of `Variable` objects to update to minimize
            `loss`, or a callable returning the list or tuple of `Variable`
            objects.  Use callable when the variable list would otherwise be
            incomplete before `minimize` since the variables are created at the
            first time `loss` is called.
          tape: (Optional) `tf.GradientTape`.

        Returns:
          None
        """
    grads_and_vars = self.compute_gradients(loss, var_list, tape)
    self.apply_gradients(grads_and_vars)

----------

def testDeferredRestorationUsageEager(self):
    """An idiomatic eager execution example."""
    num_training_steps = 10
    checkpoint_directory = self.get_temp_dir()
    checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')
    for training_continuation in range(3):
        model = MyModel()
        optimizer = tf.compat.v1.train.AdamOptimizer(0.001)
        root = tf.train.Checkpoint(optimizer=optimizer, model=model, optimizer_step=tf.compat.v1.train.get_or_create_global_step())
        root.restore(tf.train.latest_checkpoint(checkpoint_directory))
        for _ in range(num_training_steps):
            input_value = tf.constant([[3.0]])
            optimizer.minimize(lambda : model(input_value), global_step=root.optimizer_step)
        root.save(file_prefix=checkpoint_prefix)
        self.assertEqual((training_continuation + 1) * num_training_steps, root.optimizer_step.numpy())

----------



Test Class Name: CheckpointingTests