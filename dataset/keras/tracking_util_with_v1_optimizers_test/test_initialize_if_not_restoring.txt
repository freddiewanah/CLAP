@property
def variables(self):
    """Returns variables of this optimizer."""
    return CallableList(self._variables)

----------

@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))
def test_initialize_if_not_restoring(self):
    with self.test_session():
        checkpoint_directory = self.get_temp_dir()
        checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')
        optimizer_only_prefix = os.path.join(checkpoint_directory, 'opt')
        with test_utils.device(should_use_gpu=True):
            model = MyModel()
            optimizer = tf.compat.v1.train.AdamOptimizer(0.001)
            root = tf.train.Checkpoint(model=model, global_step=tf.compat.v1.train.get_or_create_global_step())
            optimizer_checkpoint = tf.train.Checkpoint(optimizer=optimizer)
            checkpoint_path = tf.train.latest_checkpoint(checkpoint_directory)
            status = root.restore(save_path=checkpoint_path)
            input_value = tf.constant([[3.0]])
            train_fn = functools.partial(optimizer.minimize, functools.partial(model, input_value), global_step=root.global_step)
            if not tf.executing_eagerly():
                train_fn = functools.partial(self.evaluate, train_fn())
            status.initialize_or_restore()
            self.evaluate([v.initializer for v in optimizer.variables()])
            train_fn()
            model_save_path = root.save(file_prefix=checkpoint_prefix)
            self.evaluate(optimizer.variables()[0].assign(42.0))
            optimizer_save_path = optimizer_checkpoint.save(optimizer_only_prefix)
        with test_utils.device(should_use_gpu=True):
            model = MyModel()
            optimizer = tf.compat.v1.train.AdamOptimizer(0.001)
            root = tf.train.Checkpoint(optimizer=optimizer, model=model, global_step=tf.compat.v1.train.get_or_create_global_step())
            status = root.restore(save_path=model_save_path)
            input_value = tf.constant([[3.0]])
            train_fn = functools.partial(optimizer.minimize, functools.partial(model, input_value), global_step=root.global_step)
            if not tf.executing_eagerly():
                train_fn = functools.partial(self.evaluate, train_fn())
            status.initialize_or_restore()
            train_fn()
            with self.assertRaises(AssertionError):
                status.assert_existing_objects_matched()
            with self.assertRaises(AssertionError):
                status.assert_consumed()
        with test_utils.device(should_use_gpu=True):
            model = MyModel()
            optimizer = tf.compat.v1.train.AdamOptimizer(0.001, beta1=1.0)
            root = tf.train.Checkpoint(optimizer=optimizer, model=model, global_step=tf.compat.v1.train.get_or_create_global_step())
            opt_root = tf.train.Checkpoint(optimizer=optimizer)
            status = root.restore(save_path=model_save_path)
            init_only_optimizer_status = opt_root.restore(save_path=None)
            optimizer_status = opt_root.restore(save_path=optimizer_save_path)
            input_value = tf.constant([[3.0]])
            train_fn = functools.partial(optimizer.minimize, functools.partial(model, input_value), global_step=root.global_step)
            if not tf.executing_eagerly():
                train_fn = functools.partial(self.evaluate, train_fn())
            optimizer_status.run_restore_ops()
            status.initialize_or_restore()
            init_only_optimizer_status.initialize_or_restore()
            train_fn()
            self.assertEqual(42.0, self.evaluate(optimizer.variables()[0]))

----------



Test Class Name: default