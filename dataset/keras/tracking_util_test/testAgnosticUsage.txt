def train_fn():
    with tf.GradientTape() as tape:
        loss = model(input_value)
    variables = model.trainable_variables
    gradients = tape.gradient(loss, variables)
    return optimizer.apply_gradients(zip(gradients, variables))

----------

@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))
def testAgnosticUsage(self):
    """Graph/eager agnostic usage."""
    with self.test_session():
        num_training_steps = 10
        checkpoint_directory = self.get_temp_dir()
        optimizer = adam.Adam(0.001)

        def _train_fn(model, input_value):
            with tf.GradientTape() as tape:
                loss = model(input_value)
            variables = model.trainable_variables
            gradients = tape.gradient(loss, variables)
            return optimizer.apply_gradients(zip(gradients, variables))
        for training_continuation in range(3):
            with test_utils.device(should_use_gpu=True):
                model = MyModel()
                root = tf.train.Checkpoint(optimizer=optimizer, model=model)
                manager = tf.train.CheckpointManager(root, checkpoint_directory, max_to_keep=1)
                status = root.restore(save_path=manager.latest_checkpoint)
                input_value = tf.constant([[3.0]])
                train_fn = functools.partial(_train_fn, model, input_value)
                if not tf.executing_eagerly():
                    train_fn = functools.partial(self.evaluate, train_fn())
                status.initialize_or_restore()
                for _ in range(num_training_steps):
                    train_fn()
                manager.save()
                self.assertEqual((training_continuation + 1) * num_training_steps, self.evaluate(root.optimizer.iterations))
                self.assertEqual(training_continuation + 1, self.evaluate(root.save_counter))

----------



Test Class Name: CheckpointingTests