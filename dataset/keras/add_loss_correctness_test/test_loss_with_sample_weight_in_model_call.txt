def evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False):
    """Returns the loss value & metrics values for the model in test mode.

        Computation is done in batches (see the `batch_size` arg.)

        Args:
            x: Input data. It could be:
              - A Numpy array (or array-like), or a list of arrays
                (in case the model has multiple inputs).
              - A TensorFlow tensor, or a list of tensors
                (in case the model has multiple inputs).
              - A dict mapping input names to the corresponding array/tensors,
                if the model has named inputs.
              - A `tf.data` dataset.
              - A generator or `keras.utils.Sequence` instance.
            y: Target data. Like the input data `x`,
              it could be either Numpy array(s) or TensorFlow tensor(s).
              It should be consistent with `x` (you cannot have Numpy inputs and
              tensor targets, or inversely).
              If `x` is a dataset, generator or
              `keras.utils.Sequence` instance, `y` should not be specified
              (since targets will be obtained from the iterator/dataset).
            batch_size: Integer or `None`.
                Number of samples per batch of computation.
                If unspecified, `batch_size` will default to 32.
                Do not specify the `batch_size` if your data is in the
                form of symbolic tensors, dataset,
                generators, or `keras.utils.Sequence` instances (since they
                generate batches).
            verbose: 0 or 1. Verbosity mode.
                0 = silent, 1 = progress bar.
            sample_weight: Optional Numpy array of weights for
                the test samples, used for weighting the loss function.
                You can either pass a flat (1D)
                Numpy array with the same length as the input samples
                (1:1 mapping between weights and samples),
                or in the case of temporal data,
                you can pass a 2D array with shape
                `(samples, sequence_length)`,
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                `sample_weight_mode="temporal"` in `compile()`. This argument is
                not supported when `x` is a dataset, instead pass sample weights
                as the third element of `x`.
            steps: Integer or `None`.
                Total number of steps (batches of samples)
                before declaring the evaluation round finished.
                Ignored with the default value of `None`.
                If x is a `tf.data` dataset and `steps` is
                None, 'evaluate' will run until the dataset is exhausted.
                This argument is not supported with array inputs.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during evaluation.
                See [callbacks](/api_docs/python/tf/keras/callbacks).
            max_queue_size: Integer. Used for generator or
                `keras.utils.Sequence` input only. Maximum size for the
                generator queue.  If unspecified, `max_queue_size` will default
                to 10.
            workers: Integer. Used for generator or `keras.utils.Sequence` input
                only. Maximum number of processes to spin up when using
                process-based threading. If unspecified, `workers` will default
                to 1. If 0, will execute the generator on the main thread.
            use_multiprocessing: Boolean. Used for generator or
                `keras.utils.Sequence` input only. If `True`, use process-based
                threading. If unspecified, `use_multiprocessing` will default to
                `False`. Note that because this implementation relies on
                multiprocessing, you should not pass non-picklable arguments to
                the generator as they can't be passed easily to children
                processes.

        Returns:
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.

        Raises:
            ValueError: in case of invalid arguments.
        """
    self._assert_built_as_v1()
    base_layer.keras_api_gauge.get_cell('evaluate').set(True)
    self._assert_compile_was_called()
    self._check_call_args('evaluate')
    func = self._select_training_loop(x)
    return func.evaluate(self, x=x, y=y, batch_size=batch_size, verbose=verbose, sample_weight=sample_weight, steps=steps, callbacks=callbacks, max_queue_size=max_queue_size, workers=workers, use_multiprocessing=use_multiprocessing)

----------

@test_combinations.run_all_keras_modes
def test_loss_with_sample_weight_in_model_call(self):

    class MyModel(Model):

        def __init__(self):
            super().__init__()
            self.bias = test_utils.Bias()

        def call(self, inputs):
            outputs = self.bias(inputs[0])
            self.add_loss(MAE()(inputs[1], outputs, inputs[2]))
            self.add_loss(tf.reduce_mean(inputs[2] * mae(inputs[1], outputs)))
            return outputs
    model = MyModel()
    model.predict([self.x, self.y, self.w])
    model.compile(optimizer_legacy.gradient_descent.SGD(0.05), run_eagerly=test_utils.should_run_eagerly())
    history = model.fit([self.x, self.y, self.w], batch_size=3, epochs=5)
    self.assertEqual(len(model.losses), 2)
    self.assertAllClose(history.history['loss'], [2.0, 1.8, 1.6, 1.4, 1.2], 0.001)
    eval_out = model.evaluate([self.x, self.y, self.w])
    self.assertAlmostEqual(eval_out, 1.0, 3)

----------



Test Class Name: TestAddLossCorrectness