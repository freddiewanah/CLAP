def Linear(in_features, out_features, bias=True):
    m = nn.Linear(in_features, out_features, bias)
    nn.init.xavier_uniform_(m.weight)
    if bias:
        nn.init.constant_(m.bias, 0.0)
    return m

----------

def test_load_state_dict(self):
    model = torch.nn.Linear(5, 5).cuda().half()
    params = list(model.parameters())
    optimizer = FairseqAdam(cfg=OmegaConf.create(vars(argparse.Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, lr=[1e-05]))), params=params)
    me_optimizer = MemoryEfficientFP16Optimizer(cfg=OmegaConf.create({'common': vars(argparse.Namespace(fp16_init_scale=1, fp16_scale_window=1, fp16_scale_tolerance=1, threshold_loss_scale=1, min_loss_scale=0.0001))}), params=params, optimizer=optimizer)
    loss = model(torch.rand(5).cuda().half()).sum()
    me_optimizer.backward(loss)
    me_optimizer.step()
    state = me_optimizer.state_dict()
    me_optimizer.load_state_dict(state)
    for (k, v) in me_optimizer.optimizer.state.items():
        self.assertTrue(k.dtype == torch.float16)
        for v_i in v.values():
            if torch.is_tensor(v_i):
                self.assertTrue(v_i.dtype == torch.float32)

----------



Test Class Name: TestMemoryEfficientFP16