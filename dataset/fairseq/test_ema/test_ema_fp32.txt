def to(t):
    return {'device': t.device, 'dtype': t.dtype}

----------

def test_ema_fp32(self):
    dtype = torch.float
    model = DummyModule().to(dtype)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    state = deepcopy(model.state_dict())
    config = EMAConfig(ema_fp32=True)
    ema = EMA(model, config)
    x = torch.randn(32)
    y = model(x.to(dtype))
    loss = y.sum()
    loss.backward()
    optimizer.step()
    ema.step(model)
    for (key, param) in model.state_dict().items():
        prev_param = state[key]
        ema_param = ema.get_model().state_dict()[key]
        if 'version' in key:
            continue
        self.assertIn(key, ema.fp32_params)
        self.assertLessEqual(torch.norm(ema_param.float() - (config.ema_decay * prev_param.float() + (1 - config.ema_decay) * param.float()).to(dtype).float()), torch.norm(ema_param.float() - (config.ema_decay * prev_param + (1 - config.ema_decay) * param).float()))
        self.assertTorchAllClose(ema_param, (config.ema_decay * prev_param.float() + (1 - config.ema_decay) * param.float()).to(dtype))

----------



Test Class Name: TestEMA