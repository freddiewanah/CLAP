def load_dataset(self, split, epoch=1, combine=False, **kwargs):
    """Load a given dataset split.

        Args:
            split (str): name of the split (e.g., train, valid, test)
        """
    dataset = self._load_dataset_split(split, epoch, combine)
    mask_whole_words = get_whole_word_mask(self.args, self.source_dictionary) if self.cfg.mask_whole_words else None
    (src_dataset, tgt_dataset) = MaskTokensDataset.apply_mask(dataset, self.source_dictionary, pad_idx=self.source_dictionary.pad(), mask_idx=self.mask_idx, seed=self.cfg.seed, mask_prob=self.cfg.mask_prob, leave_unmasked_prob=self.cfg.leave_unmasked_prob, random_token_prob=self.cfg.random_token_prob, freq_weighted_replacement=self.cfg.freq_weighted_replacement, mask_whole_words=mask_whole_words, mask_multiple_length=self.cfg.mask_multiple_length, mask_stdev=self.cfg.mask_stdev, skip_masking=self.cfg.skip_masking)
    with data_utils.numpy_seed(self.cfg.seed):
        shuffle = np.random.permutation(len(src_dataset))
    target_dataset = RightPadDataset(tgt_dataset, pad_idx=self.source_dictionary.pad())
    if self.cfg.d2v2_multi:
        dataset = self._d2v2_multi_dataset(src_dataset)
    else:
        dataset = self._regular_dataset(src_dataset, target_dataset)
    self.datasets[split] = SortDataset(dataset, sort_order=[shuffle, src_dataset.sizes])

----------

def test_masks_tokens(self):
    with TemporaryDirectory() as dirname:
        raw_file = os.path.join(dirname, 'raw')
        data = make_data(out_file=raw_file)
        vocab = build_vocab(data)
        binarizer = VocabularyDatasetBinarizer(vocab, append_eos=False)
        split = 'train'
        bin_file = os.path.join(dirname, split)
        FileBinarizer.multiprocess_dataset(input_file=raw_file, binarizer=binarizer, dataset_impl='mmap', vocab_size=len(vocab), output_prefix=bin_file)
        cfg = MaskedLMConfig(data=dirname, seed=42, mask_prob=0.5, random_token_prob=0, leave_unmasked_prob=0)
        task = MaskedLMTask(cfg, binarizer.dict)
        original_dataset = task._load_dataset_split(bin_file, 1, False)
        task.load_dataset(split)
        masked_dataset = task.dataset(split)
        mask_index = task.source_dictionary.index('<mask>')
        iterator = task.get_batch_iterator(dataset=masked_dataset, max_tokens=65536, max_positions=4096).next_epoch_itr(shuffle=False)
        for batch in iterator:
            for sample in range(len(batch)):
                net_input = batch['net_input']
                masked_src_tokens = net_input['src_tokens'][sample]
                masked_src_length = net_input['src_lengths'][sample]
                masked_tgt_tokens = batch['target'][sample]
                sample_id = batch['id'][sample]
                original_tokens = original_dataset[sample_id]
                original_tokens = original_tokens.masked_select(masked_src_tokens[:masked_src_length] == mask_index)
                masked_tokens = masked_tgt_tokens.masked_select(masked_tgt_tokens != task.source_dictionary.pad())
                assert masked_tokens.equal(original_tokens)

----------



Test Class Name: TestMaskedLM