def pad(self):
    """Helper to get index of pad symbol"""
    return self.pad_index

----------

def test_noising_dataset_without_eos(self):
    """
        Similar to test noising dataset with eos except that we have to set
        *append_eos_to_tgt* to ``True``.
        """
    (src_dict, src_tokens, _) = self._get_test_data_with_bpe_cont_marker(append_eos=False)
    src_tokens = torch.t(src_tokens)
    src_tokens_no_pad = []
    for src_sentence in src_tokens:
        src_tokens_no_pad.append(utils.strip_pad(tensor=src_sentence, pad=src_dict.pad()))
    denoising_batch_result = self._get_noising_dataset_batch(src_tokens_no_pad=src_tokens_no_pad, src_dict=src_dict, append_eos_to_tgt=True)
    (eos, pad) = (src_dict.eos(), src_dict.pad())
    expected_src = torch.LongTensor([[4, 5, 10, 11, 8, 12, 13], [pad, pad, pad, 6, 8, 9, 7]])
    expected_tgt = torch.LongTensor([[4, 5, 10, 11, 8, 12, 13, eos], [6, 7, 8, 9, eos, pad, pad, pad]])
    generated_src = denoising_batch_result['net_input']['src_tokens']
    tgt_tokens = denoising_batch_result['target']
    self.assertTensorEqual(expected_src, generated_src)
    self.assertTensorEqual(expected_tgt, tgt_tokens)

----------

def _get_test_data_with_bpe_cont_marker(self, append_eos=True):
    """
        Args:
            append_eos: if True, each input sentence in the source tokens tensor
                will have an EOS appended to the end.

        Returns:
            vocabs: BPE vocab with continuation markers as suffixes to denote
                non-end of word tokens. This is the standard BPE format used in
                fairseq's preprocessing.
            x: input tensor containing numberized source tokens, with EOS at the
                end if append_eos is true
            src_lengths: and source lengths.
        """
    vocab = Dictionary()
    vocab.add_symbol('he@@')
    vocab.add_symbol('llo')
    vocab.add_symbol('how')
    vocab.add_symbol('are')
    vocab.add_symbol('y@@')
    vocab.add_symbol('ou')
    vocab.add_symbol('n@@')
    vocab.add_symbol('ew')
    vocab.add_symbol('or@@')
    vocab.add_symbol('k')
    src_tokens = [['he@@', 'llo', 'n@@', 'ew', 'y@@', 'or@@', 'k'], ['how', 'are', 'y@@', 'ou']]
    (x, src_lengths) = (x, src_lengths) = self._convert_src_tokens_to_tensor(vocab=vocab, src_tokens=src_tokens, append_eos=append_eos)
    return (vocab, x, src_lengths)
def _get_noising_dataset_batch(self, src_tokens_no_pad, src_dict, append_eos_to_tgt=False):
    """
        Constructs a NoisingDataset and the corresponding
        ``LanguagePairDataset(NoisingDataset(src), src)``. If
        *append_eos_to_tgt* is True, wrap the source dataset in
        :class:`TransformEosDataset` to append EOS to the clean source when
        using it as the target.
        """
    src_dataset = test_utils.TestDataset(data=src_tokens_no_pad)
    noising_dataset = noising.NoisingDataset(src_dataset=src_dataset, src_dict=src_dict, seed=1234, max_word_shuffle_distance=3, word_dropout_prob=0.2, word_blanking_prob=0.2, noising_class=noising.UnsupervisedMTNoising)
    tgt = src_dataset
    language_pair_dataset = LanguagePairDataset(src=noising_dataset, tgt=tgt, src_sizes=None, src_dict=src_dict)
    language_pair_dataset = TransformEosDataset(language_pair_dataset, src_dict.eos(), append_eos_to_tgt=append_eos_to_tgt)
    dataloader = torch.utils.data.DataLoader(dataset=language_pair_dataset, batch_size=2, collate_fn=language_pair_dataset.collater)
    denoising_batch_result = next(iter(dataloader))
    return denoising_batch_result

Test Class Name: TestDataNoising