def load_model_ensemble_and_task(filenames, arg_overrides: Optional[Dict[str, Any]]=None, task=None, strict=True, suffix='', num_shards=1, state=None):
    assert state is None or len(filenames) == 1
    from fairseq import tasks
    assert not (strict and num_shards > 1), 'Cannot load state dict with strict=True and checkpoint shards > 1'
    ensemble = []
    cfg = None
    for filename in filenames:
        orig_filename = filename
        model_shard_state = {'shard_weights': [], 'shard_metadata': []}
        assert num_shards > 0
        st = time.time()
        for shard_idx in range(num_shards):
            filename = get_maybe_sharded_checkpoint_filename(orig_filename, suffix, shard_idx, num_shards)
            if not PathManager.exists(filename):
                raise IOError('Model file not found: {}'.format(filename))
            if state is None:
                state = load_checkpoint_to_cpu(filename, arg_overrides)
            if 'args' in state and state['args'] is not None:
                cfg = convert_namespace_to_omegaconf(state['args'])
            elif 'cfg' in state and state['cfg'] is not None:
                cfg = state['cfg']
            else:
                raise RuntimeError(f'Neither args nor cfg exist in state keys = {state.keys()}')
            if task is None:
                task = tasks.setup_task(cfg.task, from_checkpoint=True)
            if 'task_state' in state:
                task.load_state_dict(state['task_state'])
            argspec = inspect.getfullargspec(task.build_model)
            if 'fsdp_metadata' in state and num_shards > 1:
                model_shard_state['shard_weights'].append(state['model'])
                model_shard_state['shard_metadata'].append(state['fsdp_metadata'])
                if not has_FSDP:
                    raise ImportError('Cannot find FullyShardedDataParallel. Please install fairscale with: pip install fairscale')
                if shard_idx == num_shards - 1:
                    consolidated_model_state = FSDP.consolidate_shard_weights(shard_weights=model_shard_state['shard_weights'], shard_metadata=model_shard_state['shard_metadata'])
                    if 'from_checkpoint' in argspec.args:
                        model = task.build_model(cfg.model, from_checkpoint=True)
                    else:
                        model = task.build_model(cfg.model)
                    if 'optimizer_history' in state and len(state['optimizer_history']) > 0 and ('num_updates' in state['optimizer_history'][-1]):
                        model.set_num_updates(state['optimizer_history'][-1]['num_updates'])
                    model.load_state_dict(consolidated_model_state, strict=strict, model_cfg=cfg.model)
            else:
                if 'from_checkpoint' in argspec.args:
                    model = task.build_model(cfg.model, from_checkpoint=True)
                else:
                    model = task.build_model(cfg.model)
                if 'optimizer_history' in state and len(state['optimizer_history']) > 0 and ('num_updates' in state['optimizer_history'][-1]):
                    model.set_num_updates(state['optimizer_history'][-1]['num_updates'])
                model.load_state_dict(state['model'], strict=strict, model_cfg=cfg.model)
            state = None
            if shard_idx % 10 == 0 and shard_idx > 0:
                elapsed = time.time() - st
                logger.info(f'Loaded {shard_idx} shards in {elapsed:.2f}s, {elapsed / (shard_idx + 1):.2f}s/shard')
        ensemble.append(model)
    return (ensemble, cfg, task)

----------

def test_prune_state_dict(self):
    with contextlib.redirect_stdout(StringIO()):
        extra_args = ['--encoder-layerdrop', '0.01', '--decoder-layerdrop', '0.01']
        with self._train_transformer(seed=1, extra_args=extra_args) as model:
            (ensemble, cfg, task) = checkpoint_utils.load_model_ensemble_and_task(filenames=[model], arg_overrides={'encoder_layers_to_keep': '0,2', 'decoder_layers_to_keep': '1'})
            self.assertEqual(len(ensemble), 1)
            self.assertEqual(len(ensemble[0].encoder.layers), 2)
            self.assertEqual(len(ensemble[0].decoder.layers), 1)

----------

@contextlib.contextmanager
def _train_transformer(self, seed, extra_args=None):
    if extra_args is None:
        extra_args = []
    with tempfile.TemporaryDirectory(f'_train_transformer_seed{seed}') as data_dir:
        create_dummy_data(data_dir)
        preprocess_translation_data(data_dir)
        train_translation_model(data_dir, 'transformer_iwslt_de_en', ['--encoder-layers', '3', '--decoder-layers', '3', '--encoder-embed-dim', '8', '--decoder-embed-dim', '8', '--seed', str(seed)] + extra_args)
        yield os.path.join(data_dir, 'checkpoint_last.pt')

Test Class Name: TestCheckpointUtils