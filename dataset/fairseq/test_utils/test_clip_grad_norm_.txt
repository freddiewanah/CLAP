@torch.no_grad()
def clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:

    def grad_exists(p):
        return p is not None and getattr(p, 'grad', None) is not None
    if isinstance(params, torch.Tensor):
        params = [params]
    params = list(params)
    grads = [p.grad.detach() for p in params if grad_exists(p) and (not hasattr(p, 'expert'))]
    expert_grads = [p.grad.detach() for p in params if grad_exists(p) and hasattr(p, 'expert')]
    if len(grads) == 0:
        if len(params) > 0:
            return params[0].new_tensor(0.0)
        else:
            return torch.tensor(0.0)
    if len(grads) == 1:
        total_norm = torch.norm(grads[0], p=2, dtype=torch.float32)
    elif multi_tensor_l2norm_available:
        total_norm = multi_tensor_total_norm(grads)
    else:
        if torch.cuda.is_available():
            warnings.warn("amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library")
            device = torch.cuda.current_device()
        elif grads[0].device.type == 'xla':
            device = grads[0].device
        else:
            device = torch.device('cpu')
        total_norm = torch.norm(torch.stack([torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]))
    if aggregate_norm_fn is not None:
        total_norm = aggregate_norm_fn(total_norm)
    if max_norm > 0:
        max_norm = float(max_norm)
        clip_coef = (max_norm / (total_norm + 1e-06)).clamp_(max=1)
        torch._foreach_mul_(grads + expert_grads, clip_coef)
    return total_norm

----------

def test_clip_grad_norm_(self):
    params = torch.nn.Parameter(torch.zeros(5)).requires_grad_(False)
    grad_norm = utils.clip_grad_norm_(params, 1.0)
    self.assertTrue(torch.is_tensor(grad_norm))
    self.assertEqual(grad_norm, 0.0)
    params = [torch.nn.Parameter(torch.zeros(5)) for i in range(3)]
    for p in params:
        p.grad = torch.full((5,), fill_value=2.0)
    grad_norm = utils.clip_grad_norm_(params, 1.0)
    exp_grad_norm = torch.full((15,), fill_value=2.0).norm()
    self.assertTrue(torch.is_tensor(grad_norm))
    self.assertEqual(grad_norm, exp_grad_norm)
    grad_norm = utils.clip_grad_norm_(params, 1.0)
    self.assertAlmostEqual(grad_norm, torch.tensor(1.0))

----------



Test Class Name: TestUtils