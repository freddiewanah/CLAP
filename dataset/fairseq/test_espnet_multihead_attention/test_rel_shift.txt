def rel_shift(self, x):
    """Compute relative positional encoding.
        Args:
            x: Input tensor B X n_head X T X 2T-1
        Returns:
            torch.Tensor: Output tensor.
        """
    zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)
    x_padded = torch.cat([zero_pad, x], dim=-1)
    x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))
    x = x_padded[:, :, 1:].view_as(x)[:, :, :, :x.size(-1) // 2 + 1]
    if self.zero_triu:
        ones = torch.ones((x.size(2), x.size(3)), device=x.device)
        x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]
    return x

----------

def test_rel_shift(self):
    expected_x = torch.tensor([[[[-0.7193, -0.4033, -0.5966], [-0.8567, 1.1006, -1.0712], [-0.5663, 0.3731, -0.892]]]])
    x = self.MHA.rel_shift(self.sample_x)
    self.assertTrue(np.allclose(expected_x.cpu().detach().numpy(), x.cpu().detach().numpy(), atol=0.0001))

----------



Test Class Name: TestRelPositionMultiHeadedAttention