def forward_attention(self, value, scores, mask):
    """Compute attention context vector.
        Args:
            value: Transformed value B X n_head X T2 X d_k.
            scores: Attention score  B X n_head X T1 X T2
            mask: Mask  T2 X B
        Returns:
            torch.Tensor: Transformed value  B X T1 X d_model
                weighted by the attention score  B X T1 X T2
        """
    n_batch = value.size(0)
    if mask is not None:
        scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(2).to(bool), float('-inf'))
        self.attn = torch.softmax(scores, dim=-1)
    else:
        self.attn = torch.softmax(scores, dim=-1)
    p_attn = self.dropout(self.attn)
    x = torch.matmul(p_attn, value)
    x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)
    return self.linear_out(x)

----------

def test_forward_attention(self):
    expected_scores = torch.tensor([[[0.1627, -0.6249], [-0.2547, -0.6487], [-0.0711, -0.8545]]])
    scores = self.MHA.forward_attention(self.sample.transpose(0, 1).view(self.B, 1, self.T, self.C), self.sample_scores, mask=None)
    self.assertTrue(np.allclose(expected_scores.cpu().detach().numpy(), scores.cpu().detach().numpy(), atol=0.0001))

----------



Test Class Name: TestESPNETMultiHeadedAttention